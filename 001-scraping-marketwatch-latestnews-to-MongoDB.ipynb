{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Scrapy and MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python project was built to scrape headline titles and urls to the full news page from the latest news page of marketwatch.com. The scraped data are then be stored in MongoDB.\n",
    "\n",
    "The environment for this project is built with \n",
    "\n",
    "\n",
    "conda env create -f requirements.txt\n",
    "\n",
    "- Note: Twisted for Scrapy must <= v 16.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MarketWatch Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper can be generated with the following commands in cmd.exe:\n",
    "\n",
    "    c:\\workpath\\>scrapy startproject marketspider\n",
    "\n",
    "A scrapy spider project is created using the default spider template as follows:\n",
    "![](figures/001-raw-files.png)\n",
    "\n",
    "\n",
    "\n",
    "In **items.py** file, we define the class to store scraped data, including newsid, title, timestamp of the headline, and the url.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class MarketspiderItem(scrapy.Item):\n",
    "    newsid = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    timestamp = scrapy.Field()\n",
    "```\n",
    "\n",
    "In ** marketspider_spider.py**, we define how the data will be scraped from the given webpage. The start_url willl be 'http://www.marketwatch.com/newsviewer'. Using Chrome 'developer tools', we can find the XPath syntax for all headlines, newsids, timestamps of all news on this page. a defined `item()` is defined by `parse` function.\n",
    "- Note: The current spider only handles 80 news, which is the default number of headlines on the 1st page of the 'Latest News' tab of marketwatch.com. We will improve our scraper to crawl over all news on this **infinite scrolling tab**.\n",
    "\n",
    "```python\n",
    "from scrapy import Spider\n",
    "from scrapy.selector import Selector\n",
    "from marketspider.items import MarketspiderItem\n",
    "\n",
    "class MarketspiderSpider(Spider):\n",
    "    name = 'marketspider'\n",
    "    allowed_domains = ['marketwatch.com']\n",
    "    start_urls = [\n",
    "            'http://www.marketwatch.com/newsviewer',\n",
    "            ]\n",
    "    def parse(self, response):\n",
    "        # select timesamps and headlines in the 'latest news' tag of newsviewer\n",
    "        headlines = Selector(response).xpath('//div[@class=\"nv-text-cont\"]/h4')\n",
    "        timestamps = Selector(response).xpath('//li/@timestamp')\n",
    "        newsids = Selector(response).xpath('//li/@id')\n",
    "\n",
    "        for (newsid,timestamp, headline) in zip(newsids,timestamps,headlines):\n",
    "            item = MarketspiderItem()\n",
    "            \n",
    "            item['timestamp'] = timestamp.extract().strip()\n",
    "            item['newsid'] = newsid.extract().strip()\n",
    "            if not headline.xpath('a').extract():\n",
    "                item['title'] = headline.xpath(\n",
    "                    'text()').extract()[0].strip()\n",
    "                item['url'] = 'n/a'\n",
    "                yield item\n",
    "            else:\n",
    "                item['title'] = headline.xpath(\n",
    "                    'a[@class=\"read-more\"]/text()').extract()[0].strip()\n",
    "                item['url'] = headline.xpath(\n",
    "                    'a[@class=\"read-more\"]/@href').extract()[0]\n",
    "                yield item\n",
    "```\n",
    "\n",
    "We must set the download delay as follows in **settings.py**\n",
    "```python\n",
    "DOWNLOAD_DELAY = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Spider to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an item is returned by the spider function, it will be sent to 'item pipeline'. According to scrapy document, each item pipeline is a Python class that implements a simple method, which receive an item and perform an action over it, also deciding if the item should continue through the pipeline or be dropped and no longer processed.\n",
    "\n",
    "In this program, we would like to store items in a MongoDB database. Operations and global settings of this database are defined in **settings.py** as part of the spider's pipeline parameters, such as pipeline name, database server location and port, database name, and collection name.\n",
    "\n",
    "```pyhton\n",
    "# Configure item pipelines\n",
    "# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "    'marketspider.pipelines.MongoDBPipeline': 300,\n",
    "}\n",
    "\n",
    "MONGODB_SERVER = 'localhost'\n",
    "MONGODB_PORT = 27017\n",
    "MONGODB_DB = 'marketwatch'\n",
    "MONGODB_COLLECTION = 'news'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pymongo, we can connect to the MongoDB database and update items to our database in **pipelines.py**. The connection to database  is defined in `__init__()` function. In `process_item()` function, we use `self.collection.update()` to insert new items into the database if the 'newsid' does not exist in the database.\n",
    "\n",
    "```python\n",
    "import pymongo\n",
    "\n",
    "from scrapy.conf import settings\n",
    "from scrapy import log\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class MongoDBPipeline(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        connection = pymongo.MongoClient(\n",
    "                settings['MONGODB_SERVER'],\n",
    "                settings['MONGODB_PORT']\n",
    "                )\n",
    "        db = connection[settings['MONGODB_DB']]\n",
    "        self.collection = db[settings['MONGODB_COLLECTION']]\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        for data in item:\n",
    "            if not data:\n",
    "                raise DropItem('Missing {}'.format(data))\n",
    "            self.collection.update({'newsid':item['newsid']},dict(item),upsert=True)\n",
    "            log.msg('Headline added to MongoDB database',\n",
    "                    level=log.DEBUG, spider=spider)\n",
    "        return item\n",
    "\n",
    "class MarketspiderPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Spider\n",
    "\n",
    "Our spider can be tested as follows\n",
    "\n",
    "    c:\\workpath\\>scrapy crawl marketspider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Web Scraping With Scrapy and MongoDB\n",
    "\n",
    "https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
